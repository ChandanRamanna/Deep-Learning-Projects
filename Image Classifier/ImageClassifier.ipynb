{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYS ENG 6213 - Deep Learning and Advanced Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will be using here is CIFAR-10 which has 60000 images of shape 32x32x3 and 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). For now, we will concentrate on tensorboard and activation functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# import necessary functions\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "maybe_download_and_extract()\n",
    "train_,tr_target,_,_ = load_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#view a few examples\n",
    "img_idx = np.random.choice(40000, 9, replace=False)\n",
    "sample = train_[img_idx].reshape(9, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
    "fig, axes1 = plt.subplots(3,3,figsize=(3,3))\n",
    "i=0\n",
    "for j in range(3):\n",
    "    for k in range(3):\n",
    "        #i = np.random.choice(range(len(sample)),replace=False)\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(sample[i])\n",
    "        i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block has all the values initialized for the neural network model. Assign following names to the values:\n",
    "1. 'X to input\n",
    "2. 'Y' to labels\n",
    "3. 'w1' to weights of input layer\n",
    "4. 'b1' to bias of input layer\n",
    "5. 'w2' to weights of output layer\n",
    "6. 'b2' to bias of output layer\n",
    "\n",
    "In addition to naming the values, we can also build histograms for values in a graph using **tf.summary.histogram(name_for_histogram,value)**. This is useful to understand how values such as weights/biases change during training. Using the function given, generate summaries for weights as follows:\n",
    "1. 'W_input' for input layer weights\n",
    "2. 'W_output' for output layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare the necessary values which will be used for training.\n",
    "input_size = 32*32*3\n",
    "hidden_size = 75\n",
    "batch_size = 200\n",
    "learning_rate = 7.5e-4\n",
    "parameters = {}\n",
    "\n",
    "### Add names to values below ###\n",
    "x = tf.placeholder(tf.float32, shape=(None,input_size))\n",
    "y = tf.placeholder(tf.float32,shape=(None,10))\n",
    "parameters['w1'] = tf.Variable(tf.random_normal([input_size,hidden_size]))\n",
    "parameters['b1'] = tf.Variable(tf.random_normal([hidden_size]))\n",
    "parameters['w2'] = tf.Variable(tf.random_normal([hidden_size,10]))\n",
    "parameters['b2'] = tf.Variable(tf.random_normal([10]))\n",
    "#### End of naming values ####    \n",
    "\n",
    "### generate histograms for weights ###\n",
    "\n",
    "#### End of histogram code code ####\n",
    "\n",
    "parameters_for_later_use = parameters # you will use this for question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you named all the values in the graph, the next step is to generate scopes to cluseter operations together for better visualizations. The syntax for building a scope is: **with tf.name_scope(scope_name):**\n",
    "\n",
    "The **two_layered_nn** function below contains the forward pass of the model. Complete the code for forward pass and assign the following scopes:\n",
    "1. 'input_layer' to {hi = w1.x+b1 and ho = ReLu(hi)}\n",
    "2. 'output_layer' to {scores = w2.ho+b2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function for forward pass\n",
    "def two_layered_nn(data,hidden_size,parameters):\n",
    "    # Forward pass steps\n",
    "    ### Type your code here ####\n",
    "    \n",
    "    #### End of your code ####\n",
    "        return(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function trains the two layered neural network.\n",
    "Complete the function as per the following specifications:\n",
    "1. use softmax and cross entropy for cost\n",
    "2. use stochastic gradient descent for learning\n",
    "3. Since cost is scalar, use **tf.summary.scalar(name_for_plot,value)** to build a plot for cost.\n",
    "4. Build a plot for accuracy in a similar manner as cost is also a scalar.\n",
    "5. The **tf.summary.merge_all()** initializes all the summaries defined in the graph. Since there is no need to generate summaries for all iterations, we decided to generate for every 10 iterations. Your job is to run a session on the output given by tf.summary.merge_all() and add it to log_writer using **writer_name.add_summary(session_name,iteration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to train the neural network\n",
    "def train(x,y,learning_rate,batch_size,hidden_size,train_,tr_target):\n",
    "    \n",
    "    scores = two_layered_nn(x,hidden_size,parameters)\n",
    "    \n",
    "    # Type code for both cost and accuracy\n",
    "    ### Start your code here ###\n",
    "    \n",
    "    #### End of your code ####\n",
    "     \n",
    "    \n",
    "    # Start a session to run the graph\n",
    "    with tf.Session() as sess:\n",
    "        log_writer = tf.summary.FileWriter(\"logs/\",sess.graph) # defining the writer for tensorboard\n",
    "        merged = tf.summary.merge_all() # initialize all summaries\n",
    "        sess.run(tf.initialize_all_variables()) # initialize all variables\n",
    "        \n",
    "        for it in range(1000): \n",
    "            loss_history = []\n",
    "            # generate random batches\n",
    "            idx = np.random.choice(40000, batch_size, replace=True)\n",
    "            ex = train_[idx]\n",
    "            ey = tr_target[idx]  \n",
    "            # run a session on optimizer and cost\n",
    "            _,c = sess.run([optimizer,cost],feed_dict = {x:ex,y:ey}) \n",
    "            \n",
    "            # run a session on variable 'merged' to get summaries\n",
    "            if it%10 == 0:\n",
    "                ### Type your code here ###\n",
    "                \n",
    "                #### End of your code ####\n",
    "                log_writer.flush()\n",
    "                \n",
    "            loss_history.append(c)\n",
    "            if it%50 == 0:\n",
    "                print('iteration ',it, 'completed out of 1000, loss: ',c)\n",
    "        log_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the train function to buil and execute the graph\n",
    "start_time = datetime.datetime.now()\n",
    "train(x,y,learning_rate,batch_size,hidden_size,train_,tr_target)\n",
    "print('Total execution time: '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to command prompt and type **tensorboard --logdir=path --debug**.\n",
    "It will give you a local host link which you can copy and open in a browser (chrome preferred over firefox).\n",
    "You can now view the following:\n",
    "1. model under graphs tab\n",
    "2. plots under scalars tab\n",
    "3. histograms under histograms tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Explore activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you have been using ReLu activation function. In lecture#4, we discussed about other activation functions like sigmoid, tanh and variations of ReLu like PReLu. A few of them are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Activation Functions](activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though sigmoid and tanh are very famous activation functions, when it comes to deep learning models, ReLu or its variants are preferred. There are two main reasons for this (second being the most important one):\n",
    "1. Relu (including its variants) is mostly linear and hence it is faster than other fuctions such as tanh/sigmoids which need exponentials\n",
    "2. Gradients passing through ReLu do not get saturated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show that Relu is faster, modify your activation function for the code in Q1 to **sigmoid** and use the weights & biases stored in **parameters_for_later_use** dictionary to train the model. You will observe that the time taken will be few seconds more for sigmoid. When we deal with neural networks with hundreds of layers, this difference will be somewhat prominent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the code for Q1 using sigmoid function here ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next advantage which is more important than speed is that gradients passing through ReLu do no get saturated. In order to understand what it means, complete the **sigmoid_backward** function. Then execute the following function calls given in next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The sigmoid function for an input $x$ is given as $s(x) = \\frac{1}{1+e^{-x}}$. The derivative of sigmoid is given as $\\frac{ds(x)}{dx}= s(x)(1-s(x))$. The proof can be found at [sigmoid derivation](http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(grad):\n",
    "    \"\"\"\n",
    "    Function to calculate backward pass for sigmoid\n",
    "    Inputs: \n",
    "    grad: numpy array of any shape\n",
    "    Outputs:\n",
    "    out: derivative of grad\n",
    "    \"\"\"\n",
    "    ### Type your code here ###\n",
    "    \n",
    "    #### End of your code ####\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad=np.asarray([[0.98, 0.96, 0.31, 0],[1, 0.58, 0.01, 0.92]])\n",
    "sigmoid_backward(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the gradient values which were given as input to sigmoid_backward are mostly at the extremes i.e. near 0 or near 1 except for 0.31 and 0.58. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
